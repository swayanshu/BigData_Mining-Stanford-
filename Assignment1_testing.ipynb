{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Authors: Jessica Su, Wanzi Zhou, Pratyaksh Sharma, Dylan Liu, Ansh Shukla\n",
    "# Modified by Evan Stene for CSCI 5702/7702\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import pdb\n",
    "import unittest\n",
    "import time\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.context import SparkContext \n",
    "from pyspark.sql.session import SparkSession \n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)               #error1 resolved\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from functools import reduce\n",
    "\n",
    "def l1(u,v):\n",
    "    return int(np.sum(np.absolute(np.array(u)-np.array(v))))\n",
    "\n",
    "class LSH:\n",
    "    def __init__(self, A, k, L):\n",
    "        \"\"\"\n",
    "        Initializes the LSH object\n",
    "        A - dataframe to be searched\n",
    "        k - number of thresholds in each function\n",
    "        L - number of functions\n",
    "        \"\"\"\n",
    "        # do not edit this function!\n",
    "        #self.sc = SparkContext()\n",
    "        self.sc = sc\n",
    "        self.k = k\n",
    "        self.L = L\n",
    "        self.A = A\n",
    "        self.functions = self.create_functions()\n",
    "        self.hashed_A = self.hash_data()\n",
    "        \n",
    "    # TODO: Implement this\n",
    "    def l1(self, u, v):\n",
    "        \"\"\"\n",
    "        Finds the L1 distance between two vectors\n",
    "        u and v are 1-dimensional Row objects\n",
    "        l1_distance = |x1-x2|+|y1-y2|\n",
    "        pandas udf in pyspark\n",
    "        \"\"\"\n",
    "        return int(np.sum(np.absolute(np.array(u)-np.array(v))))\n",
    "        # raise NotImplementedError\n",
    "\n",
    "    # TODO: Implement this\n",
    "    def load_data(self, filename):\n",
    "        \"\"\"\n",
    "        Loads the data into a spark DataFrame, where each row corresponds to\n",
    "        an image patch -- this step is sort of slow.\n",
    "        Each row in the data is an image, and there are 400 columns.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.A = spark.read.csv(filename)\n",
    "            print(\"Succesfully loaded.\")\n",
    "        except:\n",
    "            print(\"Error in opening file.\")\n",
    "            \n",
    "        return\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # TODO: Implement this\n",
    "    def create_function(self, dimensions, thresholds):\n",
    "        \"\"\"\n",
    "        Creates a hash function from a list of dimensions and thresholds.\n",
    "        \"\"\"\n",
    "        def f(v):\n",
    "            # inside_vector = [v[idx] for idx in dimensions]\n",
    "            returnable = []\n",
    "            for jdx in range(len(dimensions)):\n",
    "                returnable.append(1 if v[dimensions[jdx]] >= thresholds[jdx] else 0)\n",
    "            return returnable\n",
    "            # [c1,c2,c3....,c400]\n",
    "            # [c1,c2,c3,c250]\n",
    "            \n",
    "        # this function is returning a k-bit vector of 0s and 1s\n",
    "        udf_f = F.udf(f,T.ArrayType(T.IntegerType()))\n",
    "        return udf_f\n",
    "\n",
    "    def create_functions(self, num_dimensions=400, min_threshold=0, max_threshold=255):\n",
    "        \"\"\"\n",
    "        Creates the LSH functions (functions that compute L K-bit hash keys).\n",
    "        Each function selects k dimensions (i.e. column indices of the image matrix)\n",
    "        at random, and then chooses a random threshold for each dimension, between 0 and\n",
    "        255.  For any image, if its value on a given dimension is greater than or equal to\n",
    "        the randomly chosen threshold, we set that bit to 1.  Each hash function returns\n",
    "        a length-k bit string of the form \"0101010001101001...\", and the L hash functions \n",
    "        will produce L such bit strings for each image.\n",
    "        \"\"\"\n",
    "        functions = []\n",
    "        for i in range(self.L):\n",
    "            dimensions = np.random.randint(low = 0, \n",
    "                                    high = num_dimensions,\n",
    "                                    size = self.k)\n",
    "            dimensions = [int(x) for x in list(dimensions)]\n",
    "            thresholds = np.random.randint(low = min_threshold, \n",
    "                                    high = max_threshold + 1, \n",
    "                                    size = self.k)\n",
    "            thresholds = [int(x) for x in list(dimensions)]\n",
    "            functions.append(self.create_function(dimensions, thresholds))\n",
    "        return functions\n",
    "\n",
    "    # TODO: Implement this\n",
    "    def hash_vector(self,v):\n",
    "        \"\"\"\n",
    "        Hashes an individual vector (i.e. image).  This produces an array with L\n",
    "        entries, where each entry is a string of k bits.\n",
    "        \"\"\"\n",
    "        # you will need to use self.functions for this method\n",
    "        # the expected input is an array of floats, this is the vector v\n",
    "        vector_v = spark.createDataFrame([v]).withColumn(\"vector\",F.array(*[F.col(name).cast(\"float\") for name in df_columns]))\n",
    "        \n",
    "        for idx in range(len(self.functions)):\n",
    "            vector_v = vector_v.withColumn(\"_hashed_vector_\"+str(idx),self.functions[idx](F.col(\"vector\")))\n",
    "        return vector_v.collect()[0]\n",
    "        \n",
    "        # raise NotImplementedError\n",
    "\n",
    "    # TODO: Implement this\n",
    "    def hash_data(self):\n",
    "        \"\"\"\n",
    "        Hashes the data in A, where each row is a datapoint, using the L\n",
    "        functions in 'self.functions'\n",
    "        \"\"\"\n",
    "        \"\"\" apply a function on each row of self.A\"\"\"\n",
    "        \"\"\" for each row in my dataframe self.A, return hash_vector(r)\"\"\"\n",
    "        \n",
    "        # first we transform the dataframe into a processable shape\n",
    "        df_columns = list(self.A.columns)\n",
    "        tmp_df = self.A.withColumn(\"vector\",F.array(*[F.col(name).cast(\"float\") for name in df_columns]))\n",
    "        \n",
    "        # defining and applying the udf hash_vector function to our data\n",
    "        hashed_tmp_df = tmp_df\n",
    "        for idx in range(len(self.functions)):\n",
    "             hashed_tmp_df = hashed_tmp_df.withColumn(\"_hashed_vector_\"+str(idx),self.functions[idx](F.col(\"vector\")))\n",
    "        self.hashed_A = hashed_tmp_df\n",
    "        return hashed_tmp_df\n",
    "        # you will need to use self.A for this method\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # TODO: Implement this\n",
    "    def get_candidates(self,hashed_point, query_index):\n",
    "        \"\"\"\n",
    "        Retrieve all of the points that hash to one of the same buckets \n",
    "        as the query point.  Do not do any random sampling (unlike what the first\n",
    "        part of this problem prescribes).\n",
    "        Don't retrieve a point if it is the same point as the query point.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        I want to retrieve all the points that were hashed into the same bucket as the point located in the same hashed_point bucket\n",
    "        and then select the 3L nearest neighbours. We will execute a linear search over this bucket, comparing the real values\n",
    "        using l1 distance. The query_index point \n",
    "        \"\"\"\n",
    "        idx = query_index\n",
    "        query_point = hashed_point\n",
    "        bucket = query_point[\"_hashed_vector_\"+str(idx)]\n",
    "        query_points_bucket = self.hashed_A.filter(\"CAST(_hashed_vector_\"+str(idx)+\" as string) = '\"+str(bucket)+\"'\")\n",
    "        #query_points_bucket.collect()\n",
    "        # we want to compare our query_point[vector] to each of the query_points_bucket\n",
    "        # we will give them n l1 score\n",
    "        query_points_bucket = query_points_bucket.withColumn(\"query_vector_point\",F.array(*[F.lit(x) for x in query_point[\"vector\"]]))    # based on that score, we will retrieving the top 3L\n",
    "        # query_points_bucket.collect()\n",
    "        udf_l1 = F.udf(l1,T.IntegerType())\n",
    "        scored_query_points = query_points_bucket.filter(F.col(\"vector\")!=F.col(\"query_vector_point\")).withColumn(\"score\",udf_l1(\"query_vector_point\",\"vector\"))\n",
    "        candidates = scored_query_points.sort(\"score\",ascending=False)[[\"vector\",\"score\"]].collect()[:3*self.L]\n",
    "        return candidates\n",
    "        \n",
    "        # you will need to use self.hashed_A for this method\n",
    "        # raise NotImplementedError\n",
    "\n",
    "    # TODO: Implement this\n",
    "    def lsh_search(self,query_index, num_neighbors = 10):\n",
    "        \"\"\"\n",
    "        Run the entire LSH algorithm. Query_Index is just a way to select a point that will be query itself.\n",
    "        \"\"\"\n",
    "        query_point = self.hashed_A.limit(query_index).collect()[-1]\n",
    "        \"\"\"\n",
    "        query_point will be composed of the vector and its resulting hashes with each function in self.functions\n",
    "        \"\"\"\n",
    "        result_vector = []\n",
    "        for idx in range(self.L):\n",
    "            result_vector.append(self.get_candidates(query_point,idx))\n",
    "            sqlContext.clearCache()\n",
    "\n",
    "        flattened_results = reduce(lambda x,y:x+y,result_vector)\n",
    "        sorted_results = sorted(flattened_results,key=lambda x:x[\"score\"],reverse=False)\n",
    "\n",
    "        # we will return the top 10 values.     \n",
    "        # query_index is a vector of values\n",
    "        # num_neighbors = self.L\n",
    "        return sorted_results[:num_neighbors]\n",
    "        # raise NotImplementedError\n",
    "\n",
    "# Plots images at the specified rows and saves them each to files.\n",
    "def plot(A, row_nums, base_filename):\n",
    "    for row_num in row_nums:\n",
    "        patch = np.reshape(A[row_num, :], [20, 20])\n",
    "        im = Image.fromarray(patch)\n",
    "        if im.mode != 'RGB':\n",
    "            im = im.convert('RGB')\n",
    "        im.save(base_filename + \"-\" + str(row_num) + \".png\")\n",
    "\n",
    "# Finds the nearest neighbors to a given vector, using linear search.\n",
    "# TODO: Implement this\n",
    "def linear_search(A, query_index, num_neighbors):\n",
    "    df_columns = list(A.columns)\n",
    "    tmp_df = A.withColumn(\"vector\",F.array(*[F.col(name).cast(\"float\") for name in df_columns]))\n",
    "    query_point = tmp_df.limit(query_index).collect()[-1]\n",
    "    # sqlContext.clearCache()\n",
    "    tmp_df = tmp_df.withColumn(\"query_point\",F.array(*[F.lit(x) for x in query_point[\"vector\"]]))\n",
    "    udf_l1 = F.udf(l1,T.IntegerType())\n",
    "    top_n = tmp_df.withColumn(\"score\",udf_l1(\"vector\",\"query_point\")).sort(\"score\",ascending=False).limit(num_neighbors).collect()\n",
    "    return top_n\n",
    "    # raise NotImplementedError\n",
    "\n",
    "# Write a function that computes the error measure\n",
    "# TODO: Implement this\n",
    "def lsh_error():\n",
    "    raise NotImplementedError\n",
    "\n",
    "#### TESTS #####\n",
    "\n",
    "class TestLSH(unittest.TestCase):\n",
    "    def test_l1(self):\n",
    "        u = np.array([1, 2, 3, 4])\n",
    "        v = np.array([2, 3, 2, 3])\n",
    "        self.assertEqual(l1(u, v), 4)\n",
    "\n",
    "    def test_hash_data(self):\n",
    "        f1 = lambda v: sum(v)\n",
    "        f2 = lambda v: sum([x * x for x in v])\n",
    "        A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "        self.assertEqual(f1(A[0,:]), 6)\n",
    "        self.assertEqual(f2(A[0,:]), 14)\n",
    "\n",
    "        functions = [f1, f2]\n",
    "        self.assertTrue(np.array_equal(hash_vector(functions, A[0, :]), np.array([6, 14])))\n",
    "        self.assertTrue(np.array_equal(hash_data(functions, A), np.array([[6, 14], [15, 77]])))\n",
    "\n",
    "    ### TODO: Write your tests here (they won't be graded, \n",
    "    ### but you may find them helpful)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#    unittest.main() ### TODO: Uncomment this to run tests\n",
    "    # create an LSH object using lsh = LSH(k=16, L=10)\n",
    "    lsh_model = LSH(A=spark.read.csv(\"patches.csv\"),k=16,L=10)\n",
    "    t_results=lsh_model.lsh_search(1)\n",
    "   \n",
    "    \"\"\"\n",
    "    Your code here\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_search(A, query_index, num_neighbors):\n",
    "    df_columns = list(A.columns)\n",
    "    tmp_df = A.withColumn(\"vector\",F.array(*[F.col(name).cast(\"float\") for name in df_columns]))\n",
    "    query_point = tmp_df.limit(query_index).collect()[-1]\n",
    "    # sqlContext.clearCache()\n",
    "    tmp_df = tmp_df.withColumn(\"query_point\",F.array(*[F.lit(x) for x in query_point[\"vector\"]]))\n",
    "    udf_l1 = F.udf(l1,T.IntegerType())\n",
    "    top_n = tmp_df.withColumn(\"score\",udf_l1(\"vector\",\"query_point\")).sort(\"score\",ascending=False).limit(num_neighbors).collect()\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh_model = LSH(A=spark.read.csv(\"/users/gayanigupta/Assignment_01/patches.csv\"),k=24,L=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.csv(\"/users/gayanigupta/Assignment_01/patches.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_test = [100,200,300,400,500,600,700,800,900,1000]\n",
    "for row in rows_to_test:\n",
    "    print(\"Starting linear search on row\",row)\n",
    "    start_time = time.time()\n",
    "    ls_results = linear_search(spark_df,row,3)\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    sqlContext.clearCache()\n",
    "    ls_distance_output = sum([x[\"score\"] for x in ls_results])\n",
    "    result_dict = {\n",
    "        \"row\":row,\n",
    "        \"time\":total_time,\n",
    "        \"NN\":ls_results,\n",
    "        \"distance\":ls_distance_output\n",
    "    }\n",
    "    with open(\"./assignment01_row_\"+str(row)+\"_linearsearch.pickle\",\"wb\") as f:\n",
    "        pickle.dump(result_dict,f)\n",
    "    print(\"Finished running linear search on row\",row)\n",
    "    print(\"===================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row 100, repeat for all rows\n",
    "with open(\"./assignment01_row_100_linearsearch.pickle\",\"rb\") as f:\n",
    "    denominator = pickle.load(f)[\"distance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator = results_lsh[\"distance\"]\n",
    "# all of this is for an specific case of K and L and number of neighbors\n",
    "row100_error = numerator/denominator\n",
    "row200_error\n",
    ".\n",
    ".\n",
    ".\n",
    "row1000_error\n",
    "error_sum = row100_erro + ..... + row100_error\n",
    "error_completed = error_sum/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_test = [100,200,300,400,500,600,700,800,900,1000]\n",
    "k_to_test = [16,18,20,22,24]\n",
    "l_to_test = [10,12,14,16,18,20]  \n",
    "for k in k_to_test:\n",
    "    lsh_model = LSH(A=spark.read.csv(\"/users/gayanigupta/Assignment_01/patches.csv\"),k=k,L=10)\n",
    "    for row in rows_to_test:\n",
    "        print(\"Starting test on row\",row)\n",
    "        start_time = time.time()\n",
    "        t_results=lsh_model.lsh_search(row,3) # first parameter is query_index, second parameter is num_neighbors\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        result_dict = {\n",
    "            \"row\":row,\n",
    "            \"time\":total_time,\n",
    "            \"ANN\":t_results,\n",
    "            \"distance\":sum([x[\"score\"] for x in t_results])\n",
    "        }\n",
    "        with open(\"./assignment01_row_\"+str(row)+\"_k\"+str(k)+\".pickle\",\"wb\") as f:\n",
    "            pickle.dump(result_dict,f)\n",
    "\n",
    "        print(\"Finished test on row\",row)\n",
    "        print(\"============================================================\")\n",
    "        sqlContext.clearCache()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in l_to_test:\n",
    "    lsh_model = LSH(A=spark.read.csv(\"/users/gayanigupta/Assignment_01/patches.csv\"),k=24,L=l)\n",
    "    for row in rows_to_test:\n",
    "        print(\"Starting test on row\",row)\n",
    "        start_time = time.time()\n",
    "        t_results=lsh_model.lsh_search(row,3) # first parameter is query_index, second parameter is num_neighbors\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        result_dict = {\n",
    "            \"row\":row,\n",
    "            \"time\":total_time,\n",
    "            \"ANN\":t_results,\n",
    "            \"distance\":sum([x[\"score\"] for x in t_results])\n",
    "        }\n",
    "        tested_results.append(result_dict)\n",
    "        with open(\"./assignment01_row_\"+str(row)+\"_L\"+str(l)+\".pickle\",\"wb\") as f:\n",
    "            pickle.dump(result_dict,f)\n",
    "\n",
    "        print(\"Finished test on row\",row)\n",
    "        print(\"============================================================\")\n",
    "        sqlContext.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_test = [100,200,300,400,500,600,700,800,900,1000]\n",
    "for row in rows_to_test:\n",
    "    print(\"Starting linear search on row\",row)\n",
    "    start_time = time.time()\n",
    "    ls_results = linear_search(spark_df,row,10)\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    sqlContext.clearCache()\n",
    "    ls_distance_output = sum([x[\"score\"] for x in ls_results])\n",
    "    result_dict = {\n",
    "        \"row\":row,\n",
    "        \"time\":total_time,\n",
    "        \"NN\":ls_results,\n",
    "        \"distance\":ls_distance_output\n",
    "    }\n",
    "    with open(\"./assignment01_row_\"+str(row)+\"_linearsearch_nn10.pickle\",\"wb\") as f:\n",
    "        pickle.dump(result_dict,f)\n",
    "    print(\"Finished running linear search on row\",row)\n",
    "    print(\"===================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row 100, plot the top 10 NN \n",
    "with open(\"./assignment01_row_100_linearsearch_nn10.pickle\",\"rb\") as f:\n",
    "    row100_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(np.array([x[\"vector\"] for x in row100_results[\"NN\"]]),[0,1,2,3,4,5,6,7,8,9],\"./linearsearch_top10_row100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_test = [100,200,300,400,500,600,700,800,900,1000]\n",
    "lsh_model = LSH(A=spark.read.csv(\"/users/gayanigupta/Assignment_01/patches.csv\"),k=24,L=10)\n",
    "for row in rows_to_test:\n",
    "    print(\"Starting test on row\",row)\n",
    "    start_time = time.time()\n",
    "    t_results=lsh_model.lsh_search(row,10) # first parameter is query_index, second parameter is num_neighbors\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    result_dict = {\n",
    "        \"row\":row,\n",
    "        \"time\":total_time,\n",
    "        \"ANN\":t_results,\n",
    "        \"distance\":sum([x[\"score\"] for x in t_results])\n",
    "    }\n",
    "    with open(\"./assignment01_row_\"+str(row)+\"_nn10.pickle\",\"wb\") as f:\n",
    "        pickle.dump(result_dict,f)\n",
    "\n",
    "    print(\"Finished test on row\",row)\n",
    "    print(\"============================================================\")\n",
    "    sqlContext.clearCache()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./assignment01_row_100_nn10.pickle\",\"rb\") as f:\n",
    "    row100_lsh_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(np.array([x[\"vector\"] for x in row100_lsh_results[\"NN\"]]),[0,1,2,3,4,5,6,7,8,9],\"./lsh_top10_row100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh_model.hashed_A[[\"_hashed_vector_2\"]].limit(5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.array([1, 2, 3, 4])\n",
    "v = np.array([2, 3, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.absolute(u-v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduce(lambda x,y:x+y,np.absolute(u-v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " spark_df = spark.read.csv(\"/users/gayanigupta/Assignment_01/patches.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.limit(1)[[\"_c0\"]].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_function(r):\n",
    "    return r>=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "spark_df.limit(1).withColumn(\"test\",test_function(F.col(\"_c0\")))[[\"test\"]].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = []\n",
    "num_dimensions = 10\n",
    "k = 5\n",
    "min_threshold = 1\n",
    "max_threshold = 4\n",
    "for i in range(10):\n",
    "    dimensions = np.random.randint(low = 0, \n",
    "                            high = num_dimensions,\n",
    "                            size = k)\n",
    "    thresholds = np.random.randint(low = min_threshold, \n",
    "                            high = max_threshold + 1, \n",
    "                            size = k)\n",
    "    print(dimensions,thresholds)\n",
    "    #functions.append(create_function(dimensions, thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [c1,c2,c3,c4,c5,c6,c7,c8] each e >= 2  [0,0,1,1,1,0,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inside_vector = np.array([1,2,3,4,5,6,7,8,])[np.array([2,3,4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inside_vector >= inside_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_f(x):\n",
    "    # print(x)\n",
    "    return [float(y)+1 for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[test_f(np.array(list(r.values()))) for r in spark_df.limit(2).toPandas().to_dict(\"records\")][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns =list(spark_df.columns)\n",
    "df_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = spark_df.withColumn(\"vector\",F.array(*[F.col(name).cast(\"float\") for name in df_columns]))[[\"vector\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_functions_creation():\n",
    "    functions = []\n",
    "    for idx in range(3):\n",
    "        \n",
    "        def f(u):\n",
    "            dimensions = [int(random.randint(0,399)) for x in list(range(10))]\n",
    "            #print(dimensions)\n",
    "            thresholds = [int(random.randint(40,180)) for x in list(range(10))]\n",
    "            #print(thresholds)\n",
    "            ds = copy.deepcopy(dimensions)\n",
    "            ts = copy.deepcopy(thresholds)\n",
    "            result_vector = []\n",
    "            for jdx in range(10):\n",
    "                result_vector.append(1 if u[ds[jdx]]>=ts[jdx] else 0)\n",
    "            return result_vector\n",
    "        functions.append(f)\n",
    "    return functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_functions = test_functions_creation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_functions = []\n",
    "for f in test_functions:\n",
    "    # print(f([100,100,100,100,100,100,100,100,100,100,100,100,100,100]))\n",
    "    f_udf = F.udf(f,T.ArrayType(T.IntegerType()))\n",
    "    udf_functions.append(f_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hash_data function\n",
    "hashed_tmp_df = tmp_df\n",
    "for idx in range(len(udf_functions)):\n",
    "    # hash_vector function\n",
    "    hashed_tmp_df = hashed_tmp_df.withColumn(\"_hashed_vector_\"+str(idx),udf_functions[idx](F.col(\"vector\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_index = 10\n",
    "query_point = hashed_tmp_df.limit(query_index).collect()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = spark.createDataFrame([{\"vector\":[0,1,1,1,1,1]}])\n",
    "for idx in range(len(udf_functions)):\n",
    "    t = t.withColumn() # same parameters as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_l1(u,v):\n",
    "    return int(np.sum(np.absolute(np.array(u)-np.array(v))))\n",
    "\n",
    "udf_l1 = F.udf(test_l1,T.IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "query_point will be composed of the vector and its resulting hashes with each function in self.functions\n",
    "\"\"\"\n",
    "result_vector = []\n",
    "L = len(udf_functions)\n",
    "for idx in range(3):\n",
    "    bucket  = query_point[\"_hashed_vector_\"+str(idx)]\n",
    "    # query points in bucket\n",
    "    query_points_bucket = hashed_tmp_df.filter(\"CAST(_hashed_vector_\"+str(idx)+\" as string) = '\"+str(bucket)+\"'\")\n",
    "    #query_points_bucket.collect()\n",
    "    # we want to compare our query_point[vector] to each of the query_points_bucket\n",
    "    # we will give them n l1 score\n",
    "    query_points_bucket = query_points_bucket.withColumn(\"query_vector_point\",F.array(*[F.lit(x) for x in query_point[\"vector\"]]))    # based on that score, we will retrieving the top 3L\n",
    "    #query_points_bucket.collect()\n",
    "    # udf_l1 = F.udf(udf_l1,T.ArrayType(T.FloatType()))\n",
    "    scored_query_points = query_points_bucket.filter(F.col(\"vector\")!=F.col(\"query_vector_point\")).withColumn(\"score\",udf_l1(\"query_vector_point\",\"vector\"))\n",
    "    candidates = scored_query_points.sort(\"score\",ascending=False)[[\"vector\",\"score\"]].collect()[:3*L]\n",
    "    result_vector.append(candidates)\n",
    "\n",
    "flattened_results = reduce(lambda x,y:x+y,result_vector)\n",
    "sorted_results = sorted(flattened_results,key=lambda x:x[\"score\"],reverse=False)\n",
    "\n",
    "# we will return the top 10 values.     \n",
    "# query_index is a vector of values\n",
    "num_neighbors = 10\n",
    "print(\"Query point\",query_point)\n",
    "print(\"Results:\")\n",
    "print(len(sorted_results[:num_neighbors]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x[\"score\"] for x in sorted_results[:num_neighbors]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashed_tmp_df.groupby(\"_hashed_vector_0\").count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Query point\",query_point)\n",
    "print(\"Results:\")\n",
    "print(sorted_results[:num_neighbors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df.withColumn(\"hashed_vector\",udf_hash_vector(F.col(\"vector\")))[[\"hashed_vector\"]].collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
